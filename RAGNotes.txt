RAG = Retrieval + Augmented + Generation
A smart way to give llm access to external knowledge so they can give better, more accurate answers.

User: "Tell me about a product XXXXXXXXXX"

Retriever - Searches company docs, pdfs or a vector database
Augmentor - picks the most relevant chunks of text
Generator(LLM) - writes an answer using that retrieved knowledge.
More accurate, up-to-date, and personalized response!

Simple prompting - like asking someone to answer without letting them google or look at their notes.

With RAG implemented,
    you give a prompt
    RAG searches a knowledge base for relevant documents or information
    these fetched data/documents are added as context to the LLM
    then the LLM generates a more accurate and grounded answer

RAG Flow:
User -> Retriever(R) ->Search & Fetch Context -> prompt + context(A) -> LLM(G)

We need to load all documents into vector store/db as pre req. this is one time activity.

Vector Store -
is a special type of database to store and search data in form of vectors(high dimensional numerical representation)
vector search understands context and meaning, enabling semantic search.
popular tools - Azure AI Search, mongodb, pinecone, qdrant and redis...

1. indexing the knowledge
    convert docs into vector embeddings
    store in vector store
2. semantic retrieval
    when a user asks a question:
    convert it into a query vector
    use the vector store to retrieve most similar document chunks
3. augment the prompt
    inject the retrieved chunks into LLM support
    The model uses this relevant context to generate accurate answers

Apache tika - tool to read text from different types of files ex:- pdf

RAG - DOCUMENT
we need to make sure splitting large documents into chunks before storing it into vector store. if it is stored as document for every request whole document will be sent as input context which will cost fortune.

we can optimize token usage by splitting documents into chunks.

RAG Flow with Web search
till now, the required context information is retrieved from the private documents by using vector.
but now, we are going to fetch the relevant contextual information from web.

** whatever core llm models that we are trying to integrate, they are not capable of reading the latest information from the web, they are always going to have a cutoff date.

to implement we need to invoke search engine api from our Spring AI
tavily - we can use the apis exposed by tavily to get real time data from web

Naive RAG:
a simple rag approach where retrieved documents are directly passed to the LLM without optimization, which may lead to irrelevant or redundant context
Retriever -> Augmentation -> Generation

Advanced RAG:
it applies techniques like query rewriting, filtering, ranking and summarization before and after retrieval to deliver more accurate, relevant and concise results.
Prompt -> Pre-Retrieval -> Retriever -> Post-Retrieval -> Augmentation -> Generation

Pre-Retrieval -
used to transform the input query into a more effective form
helps to handle poorly structured queries, simplifies complex vocabulary
we can translate any type of input language to target language for accurate retrieval
can also adapt queries across unsupported languages for better search results

queryTransformers() - used in Spring AI for pre-retrieval implementations
CompressionQueryTransformer - useful to compress a conversation history and a follow-up query into a standalone query.
RewriteQueryTransformer - to rewrite a user query to provide better results when querying a target system, such as a vector store or a web search engine.
TranslationQueryTransformer - to translate a query to a target language that is supported by the embedding model to generate the document embeddings.

Post-Retrieval -
just after retrieving the relevant documents, we can check for any sensitive information present. if yes, we can mask those details and forward the information to llm.
it refines retrieved documents before augmentation
it helps to tackle issues like "lost-in-the-middle", context length limits and redundant data.
examples include ranking relevance, compressing content, or masking sensitive information like emails, phone numbers...

documentPostProcessors() - method used after documents retriever to mask sensitive information before sending to llm
we can create our own custom document post processor by implementing DocumentPostProcessor class and implementation using builder pattern
Ex - public class PIIMaskingDocumentPostProcessor implements DocumentPostProcessor












